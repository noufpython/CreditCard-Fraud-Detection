# -*- coding: utf-8 -*-
"""CreditCard_Fraud_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AhjYy0vRlzg8fDH1OlSDhqUpNir3AkAI
"""

import os

base_dir = "Project_Fraud_Detection"

folders = [
    f"{base_dir}/Data/original_data",
    f"{base_dir}/Data/preprocessed_data",
    f"{base_dir}/Data/Results"
]

for folder in folders:
    os.makedirs(folder, exist_ok=True)

print("Folder structure created successfully.")

from google.colab import files
uploaded = files.upload()

import os
import zipfile

os.makedirs("/root/.kaggle", exist_ok=True)
!mv kaggle.json /root/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d mlg-ulb/creditcardfraud

import os
import zipfile

os.makedirs("Project_Fraud_Detection/Data/original_data", exist_ok=True)

with zipfile.ZipFile("creditcardfraud.zip", 'r') as zip_ref:
    zip_ref.extractall("Project_Fraud_Detection/Data/original_data")

!ls Project_Fraud_Detection/Data/original_data

import pandas as pd

df = pd.read_csv("Project_Fraud_Detection/Data/original_data/creditcard.csv")

df.head()

import pandas as pd

pd.DataFrame().to_csv(f"{base_dir}/Data/preprocessed_data/X.csv", index=False)
pd.DataFrame().to_csv(f"{base_dir}/Data/preprocessed_data/Y.csv", index=False)
pd.DataFrame().to_csv(f"{base_dir}/Data/preprocessed_data/X_test.csv", index=False)
pd.DataFrame().to_csv(f"{base_dir}/Data/preprocessed_data/Y_test.csv", index=False)

models = ['RF', 'SVM', 'LogReg']
for model in models:
    pd.DataFrame().to_csv(f"{base_dir}/Data/Results/predictions_{model}_model.csv", index=False)

print("‚úÖ Preprocessed and results files initialized (empty).")

readme_content = """
# Credit Card Fraud Detection Project

This repository contains the full implementation of a machine learning pipeline to detect fraudulent credit card transactions.

## Folder Structure:
- `Data/original_data/`: Contains the original Kaggle dataset.
- `Data/preprocessed_data/`: Contains training/testing splits (X, Y).
- `Data/Results/`: Contains prediction outputs from all models.

## Main Files:
- `code.py`: Full Python code for preprocessing, training, and evaluation.
- `report.pdf`: Final PDF report submitted for grading.
"""

with open(f"{base_dir}/README.md", "w") as f:
    f.write(readme_content)

with open(f"{base_dir}/code.py", "w") as f:
    f.write("# Full Python project code will go here.\n")

print("‚úÖ README.md and code.py created.")

from sklearn.preprocessing import StandardScaler
df = pd.read_csv("Project_Fraud_Detection/Data/original_data/creditcard.csv")

scaler = StandardScaler()
df['scaled_amount'] = scaler.fit_transform(df['Amount'].values.reshape(-1, 1))
df['scaled_time'] = scaler.fit_transform(df['Time'].values.reshape(-1, 1))
df.drop(['Time', 'Amount'], axis=1, inplace=True)

from sklearn.model_selection import train_test_split

X = df.drop('Class', axis=1)
y = df['Class']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42)

from imblearn.over_sampling import SMOTE

sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

print("before SMOTE:", y_train.value_counts())
print("after SMOTE:", y_train_res.value_counts())

!pip install imbalanced-learn

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE

df = pd.read_csv("Project_Fraud_Detection/Data/original_data/creditcard.csv")

data = df.copy()

X = data.drop("Class", axis=1)
Y = data["Class"]

scaler = StandardScaler()
X[['Time', 'Amount']] = scaler.fit_transform(X[['Time', 'Amount']])

X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.2, random_state=42, stratify=Y)

smote = SMOTE(random_state=42)
X_train_resampled, Y_train_resampled = smote.fit_resample(X_train, Y_train)

X_train_resampled.shape, Y_train_resampled.shape, X_test.shape, Y_test.shape

print("Original Y_train distribution:\n", Y_train.value_counts())
print("Resampled Y_train distribution:\n", pd.Series(Y_train_resampled).value_counts())

print("Training set shape before SMOTE:", X_train.shape, Y_train.shape)
print("Training set shape after SMOTE:", X_train_resampled.shape, Y_train_resampled.shape)

import pandas as pd
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import StandardScaler
import os

df = pd.read_csv("Project_Fraud_Detection/Data/original_data/creditcard.csv")

X = df.drop(columns=['Class'])
y = df['Class']

scaler = StandardScaler()
X[['Time', 'Amount']] = scaler.fit_transform(X[['Time', 'Amount']])

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

os.makedirs("Project_Fraud_Detection/Data/preprocessed_data", exist_ok=True)

X_train_res.to_csv("Project_Fraud_Detection/Data/preprocessed_data/X.csv", index=False)
y_train_res.to_csv("Project_Fraud_Detection/Data/preprocessed_data/Y.csv", index=False)
X_test.to_csv("Project_Fraud_Detection/Data/preprocessed_data/X_test.csv", index=False)
y_test.to_csv("Project_Fraud_Detection/Data/preprocessed_data/Y_test.csv", index=False)

print("‚úÖ Effective training and testing files are ready!")

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import os

# Load preprocessed training and testing data
X_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X.csv")
y_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y.csv")
X_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X_test.csv")
y_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y_test.csv")

# Train the model
dt_model = DecisionTreeClassifier(random_state=42)
dt_model.fit(X_train, y_train)

# Make predictions
y_pred = dt_model.predict(X_test)
y_pred_proba = dt_model.predict_proba(X_test)[:, 1]

# Evaluate performance
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)

# Print the results
print("üéØ Decision Tree Performance:")
print(f"Accuracy:  {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")
print(f"ROC AUC:   {roc_auc:.4f}")

# Save predictions in the Results folder
os.makedirs("Project_Fraud_Detection/Data/Results", exist_ok=True)
results_df = pd.DataFrame({'Predicted': y_pred})
results_df.to_csv("Project_Fraud_Detection/Data/Results/predictions_DecisionTree_model.csv", index=False)

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import os

# Load preprocessed data
X_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X.csv")
y_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y.csv")
X_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X_test.csv")
y_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y_test.csv")

# Train the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, class_weight='balanced')
rf_model.fit(X_train, y_train)

# Make predictions
y_pred_rf = rf_model.predict(X_test)
y_proba_rf = rf_model.predict_proba(X_test)[:, 1]  # For later use in ROC AUC calculation

# Compute evaluation metrics
accuracy = accuracy_score(y_test, y_pred_rf)
precision = precision_score(y_test, y_pred_rf)
recall = recall_score(y_test, y_pred_rf)
f1 = f1_score(y_test, y_pred_rf)
roc_auc = roc_auc_score(y_test, y_proba_rf)

# Print results in a professional format
print("\n Random Forest Classifier - Evaluation Results:")
print(f"üîπ Accuracy:  {accuracy:.4f}")
print(f"üîπ Precision: {precision:.4f}")
print(f"üîπ Recall:    {recall:.4f}")
print(f"üîπ F1 Score:  {f1:.4f}")
print(f"üîπ ROC AUC:   {roc_auc:.4f}")

# Save predictions to a CSV file
os.makedirs("Project_Fraud_Detection/Data/Results", exist_ok=True)
results_rf = pd.DataFrame({'Predicted': y_pred_rf})
results_rf.to_csv("Project_Fraud_Detection/Data/Results/predictions_RF_model.csv", index=False)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, RocCurveDisplay

conf_matrix = confusion_matrix(y_test, y_pred_rf)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap="Blues")
plt.title("Confusion Matrix - Random Forest")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()


RocCurveDisplay.from_estimator(rf_model, X_test, y_test)
plt.title("ROC Curve - Random Forest")
plt.show()

import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import os

# Load the data
X_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X.csv")
y_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y.csv")
X_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X_test.csv")
y_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y_test.csv")

# Train the Naive Bayes model
nb_model = GaussianNB()
nb_model.fit(X_train, y_train)

# Make predictions
y_pred_nb = nb_model.predict(X_test)
y_proba_nb = nb_model.predict_proba(X_test)[:, 1]

# Compute evaluation metrics
accuracy = accuracy_score(y_test, y_pred_nb)
precision = precision_score(y_test, y_pred_nb)
recall = recall_score(y_test, y_pred_nb)
f1 = f1_score(y_test, y_pred_nb)
roc_auc = roc_auc_score(y_test, y_proba_nb)

# Print evaluation results
print("\nüìä Naive Bayes Classifier - Evaluation Results:")
print(f"üîπ Accuracy:  {accuracy:.4f}")
print(f"üîπ Precision: {precision:.4f}")
print(f"üîπ Recall:    {recall:.4f}")
print(f"üîπ F1 Score:  {f1:.4f}")
print(f"üîπ ROC AUC:   {roc_auc:.4f}")

# Save predictions
os.makedirs("Project_Fraud_Detection/Data/Results", exist_ok=True)
pd.DataFrame({'Predicted': y_pred_nb}).to_csv("Project_Fraud_Detection/Data/Results/predictions_NB_model.csv", index=False)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, RocCurveDisplay

# Compute the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_nb)

# Plot the confusion matrix as a heatmap
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap="Oranges")
plt.title("Confusion Matrix - Naive Bayes")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# Plot the ROC curve using the trained Naive Bayes model
RocCurveDisplay.from_estimator(nb_model, X_test, y_test)
plt.title("ROC Curve - Naive Bayes Classifier")
plt.show()

import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import os

# Load the data
X_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X.csv")
y_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y.csv")
X_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X_test.csv")
y_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y_test.csv")

# Train the Logistic Regression model
lr_model = LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42)
lr_model.fit(X_train, y_train)

# Make predictions
y_pred_lr = lr_model.predict(X_test)
y_proba_lr = lr_model.predict_proba(X_test)[:, 1]

# Compute evaluation metrics
accuracy = accuracy_score(y_test, y_pred_lr)
precision = precision_score(y_test, y_pred_lr)
recall = recall_score(y_test, y_pred_lr)
f1 = f1_score(y_test, y_pred_lr)
roc_auc = roc_auc_score(y_test, y_proba_lr)

# Print evaluation results
print("\nüìä Logistic Regression - Evaluation Results:")
print(f"üîπ Accuracy:  {accuracy:.4f}")
print(f"üîπ Precision: {precision:.4f}")
print(f"üîπ Recall:    {recall:.4f}")
print(f"üîπ F1 Score:  {f1:.4f}")
print(f"üîπ ROC AUC:   {roc_auc:.4f}")

# Save predictions
os.makedirs("Project_Fraud_Detection/Data/Results", exist_ok=True)
pd.DataFrame({'Predicted': y_pred_lr}).to_csv("Project_Fraud_Detection/Data/Results/predictions_LR_model.csv", index=False)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, RocCurveDisplay

# Plot the confusion matrix as a heatmap
conf_matrix = confusion_matrix(y_test, y_pred_lr)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap="Greens")
plt.title("Confusion Matrix - Logistic Regression")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# Plot the ROC curve using the trained Naive Bayes model
RocCurveDisplay.from_estimator(lr_model, X_test, y_test)
plt.title("ROC Curve - Logistic Regression")
plt.show()

import pandas as pd
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import os

# Load the data
X_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X.csv")
y_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y.csv")
X_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X_test.csv")
y_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y_test.csv")

# Train the KNN model (k=5)
knn_model = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)
knn_model.fit(X_train, y_train)

# Make predictions
y_pred_knn = knn_model.predict(X_test)
y_proba_knn = knn_model.predict_proba(X_test)[:, 1]

# Compute evaluation metrics
accuracy = accuracy_score(y_test, y_pred_knn)
precision = precision_score(y_test, y_pred_knn)
recall = recall_score(y_test, y_pred_knn)
f1 = f1_score(y_test, y_pred_knn)
roc_auc = roc_auc_score(y_test, y_proba_knn)

# Print evaluation results
print("\nüìä K-Nearest Neighbors - Evaluation Results:")
print(f"üîπ Accuracy:  {accuracy:.4f}")
print(f"üîπ Precision: {precision:.4f}")
print(f"üîπ Recall:    {recall:.4f}")
print(f"üîπ F1 Score:  {f1:.4f}")
print(f"üîπ ROC AUC:   {roc_auc:.4f}")

# Save predictions
os.makedirs("Project_Fraud_Detection/Data/Results", exist_ok=True)
pd.DataFrame({'Predicted': y_pred_knn}).to_csv("Project_Fraud_Detection/Data/Results/predictions_KNN_model.csv", index=False)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, RocCurveDisplay

# Compute and display the confusion matrix for KNN model
conf_matrix = confusion_matrix(y_test, y_pred_knn)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap="Purples")  # 'annot=True' shows numbers in cells
plt.title("Confusion Matrix - KNN")  # Title of the plot
plt.xlabel("Predicted Label")  # X-axis label
plt.ylabel("True Label")      # Y-axis label
plt.show()

# Display ROC curve using the KNN model and test data
RocCurveDisplay.from_estimator(knn_model, X_test, y_test)  # Automatically plots ROC curve
plt.title("ROC Curve - KNN Classifier")  # Title of the ROC plot
plt.show()

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Load the preprocessed training and testing datasets
X_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X.csv")
y_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y.csv")
X_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X_test.csv")
y_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y_test.csv")

# Flatten the label arrays if they are single-column DataFrames
y_train = y_train.values.ravel()
y_test = y_test.values.ravel()

# Build the ANN model
model = Sequential()
model.add(Dense(32, activation='relu', input_shape=(X_train.shape[1],)))  # Input layer with 32 neurons
model.add(Dropout(0.3))  # Dropout to reduce overfitting
model.add(Dense(16, activation='relu'))  # Hidden layer with 16 neurons
model.add(Dropout(0.3))  # Another dropout layer
model.add(Dense(1, activation='sigmoid'))  # Output layer with sigmoid activation for binary classification

# Compile the model with binary cross-entropy loss and Adam optimizer
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model using 20% of training data for validation
history = model.fit(X_train, y_train, epochs=10, batch_size=256, validation_split=0.2, verbose=1)

# Predict class labels (0 or 1) and prediction probabilities
y_pred_ann = (model.predict(X_test) > 0.5).astype("int32").flatten()
y_proba_ann = model.predict(X_test).flatten()

# Evaluate performance using common classification metrics
accuracy = accuracy_score(y_test, y_pred_ann)
precision = precision_score(y_test, y_pred_ann)
recall = recall_score(y_test, y_pred_ann)
f1 = f1_score(y_test, y_pred_ann)
roc_auc = roc_auc_score(y_test, y_proba_ann)

# Print evaluation results
print("\nüìä ANN Classifier - Evaluation Results:")
print(f"üîπ Accuracy:  {accuracy:.4f}")
print(f"üîπ Precision: {precision:.4f}")
print(f"üîπ Recall:    {recall:.4f}")
print(f"üîπ F1 Score:  {f1:.4f}")
print(f"üîπ ROC AUC:   {roc_auc:.4f}")

# Save predictions to CSV file
results_ann = pd.DataFrame({'Predicted': y_pred_ann})
results_ann.to_csv("Project_Fraud_Detection/Data/Results/predictions_ANN_model.csv", index=False)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, RocCurveDisplay

# ŸÖÿµŸÅŸàŸÅÿ© ÿßŸÑÿßŸÑÿ™ÿ®ÿßÿ≥
conf_matrix = confusion_matrix(y_test, y_pred_ann)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap="Reds")
plt.title("Confusion Matrix - ANN")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# ŸÖŸÜÿ≠ŸÜŸâ ROC
RocCurveDisplay.from_predictions(y_test, y_proba_ann)
plt.title("ROC Curve - ANN")
plt.show()

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import os

# Load the datasets
X_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X.csv")
y_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y.csv")
X_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X_test.csv")
y_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y_test.csv")

# Convert y to 1D array (flatten) to avoid warnings and for compatibility with sklearn
y_train = y_train.values.ravel()
y_test = y_test.values.ravel()

# Build the ANN model
model = Sequential()
model.add(Dense(32, activation='relu', input_shape=(X_train.shape[1],)))  # Input layer with 32 neurons and ReLU activation
model.add(Dropout(0.3))  # Dropout layer to reduce overfitting, drops 30% of neurons randomly
model.add(Dense(16, activation='relu'))  # Hidden layer with 16 neurons
model.add(Dropout(0.3))  # Dropout layer
model.add(Dense(1, activation='sigmoid'))  # Output layer with 1 neuron and sigmoid activation for binary classification

# Compile the model with Adam optimizer and binary cross-entropy loss
model.compile(optimizer=Adam(learning_rate=0.001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model for 10 epochs with batch size 256 and 20% validation split
history = model.fit(X_train, y_train, epochs=10, batch_size=256, validation_split=0.2, verbose=1)

# Predict on the test set, classify as 1 if probability > 0.5
y_pred_ann = (model.predict(X_test) > 0.5).astype("int32").flatten()
y_proba_ann = model.predict(X_test).flatten()  # Predicted probabilities for ROC AUC calculation

# Evaluate performance metrics
accuracy = accuracy_score(y_test, y_pred_ann)
precision = precision_score(y_test, y_pred_ann)
recall = recall_score(y_test, y_pred_ann)
f1 = f1_score(y_test, y_pred_ann)
roc_auc = roc_auc_score(y_test, y_proba_ann)

# Print evaluation results
print("\nüìä ANN Classifier - Evaluation Results:")
print(f"üîπ Accuracy:  {accuracy:.4f}")
print(f"üîπ Precision: {precision:.4f}")
print(f"üîπ Recall:    {recall:.4f}")
print(f"üîπ F1 Score:  {f1:.4f}")
print(f"üîπ ROC AUC:   {roc_auc:.4f}")

# Save the predictions to a CSV file, creating directory if it does not exist
results_ann = pd.DataFrame({'Predicted': y_pred_ann})
os.makedirs("Project_Fraud_Detection/Data/Results", exist_ok=True)
results_ann.to_csv("Project_Fraud_Detection/Data/Results/predictions_ANN_model.csv", index=False)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, RocCurveDisplay

# Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_ann)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap="Reds")
plt.title("Confusion Matrix - ANN")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# ROC curve
RocCurveDisplay.from_predictions(y_test, y_proba_ann)
plt.title("ROC Curve - ANN")
plt.show()

import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import os

# Load data
X_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X.csv")
y_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y.csv")
X_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X_test.csv")
y_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y_test.csv")

# Train Linear Regression
lr_reg_model = LinearRegression()
lr_reg_model.fit(X_train, y_train)

# Predict probabilities
y_proba_reg = lr_reg_model.predict(X_test)

# Convert predictions to binary classification using a threshold of 0.5
y_pred_reg = (y_proba_reg > 0.5).astype(int)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred_reg)
precision = precision_score(y_test, y_pred_reg)
recall = recall_score(y_test, y_pred_reg)
f1 = f1_score(y_test, y_pred_reg)
roc_auc = roc_auc_score(y_test, y_proba_reg)

# Print results
print("\nüìä Linear Regression - Evaluation Results:")
print(f"üîπ Accuracy:  {accuracy:.4f}")
print(f"üîπ Precision: {precision:.4f}")
print(f"üîπ Recall:    {recall:.4f}")
print(f"üîπ F1 Score:  {f1:.4f}")
print(f"üîπ ROC AUC:   {roc_auc:.4f}")

# Save predictions to CSV file (after flattening results)
os.makedirs("Project_Fraud_Detection/Data/Results", exist_ok=True)
pd.DataFrame({'Predicted': np.ravel(y_pred_reg)}).to_csv("Project_Fraud_Detection/Data/Results/predictions_LinearRegression_model.csv", index=False)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, RocCurveDisplay

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred_reg)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap="gray")
plt.title("Confusion Matrix - Linear Regression")
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.show()

# ROC Curve
RocCurveDisplay.from_predictions(y_test, y_proba_reg)
plt.title("ROC Curve - Linear Regression")
plt.show()

import pandas as pd
import os
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import matplotlib.pyplot as plt

# Define model names and their corresponding prediction files
models_info = {
    "Decision Tree": "predictions_DecisionTree_model.csv",
    "Random Forest": "predictions_RF_model.csv",
    "Naive Bayes": "predictions_NB_model.csv",
    "Logistic Regression": "predictions_LR_model.csv",
    "KNN": "predictions_KNN_model.csv",
    "ANN": "predictions_ANN_model.csv",
    "SVM": "predictions_SVM_model.csv",
    "Linear Regression": "predictions_LinearRegression_model.csv"
}

# Load the true y_test values
y_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y_test.csv").values.ravel()

# List to collect results
results = []

# Loop through each model and calculate metrics
for model_name, file_name in models_info.items():
    file_path = f"Project_Fraud_Detection/Data/Results/{file_name}"

    if os.path.exists(file_path):
        try:
            df_pred = pd.read_csv(file_path)
            if df_pred.empty:
                print(f"‚ö†Ô∏è Empty file: {file_name}")
                continue

            y_pred = df_pred.values.ravel()
            acc = accuracy_score(y_test, y_pred)
            prec = precision_score(y_test, y_pred, zero_division=0)
            rec = recall_score(y_test, y_pred, zero_division=0)
            f1 = f1_score(y_test, y_pred, zero_division=0)
            auc = roc_auc_score(y_test, y_pred)

            results.append([model_name, acc, prec, rec, f1, auc])
        except Exception as e:
            print(f"‚ùå Error reading {file_name}: {e}")
    else:
        print(f"üö´ File not found: {file_name}")

# Create comparison table
df_comparison = pd.DataFrame(results, columns=["Model", "Accuracy", "Precision", "Recall", "F1 Score", "ROC AUC"])
df_comparison = df_comparison.sort_values(by="ROC AUC", ascending=False).reset_index(drop=True)

# Print results as text
print("\nModel Performance Comparison:")
print(df_comparison.to_string(index=False))

# Bar plot to compare ROC AUC scores
plt.figure(figsize=(10, 6))
plt.barh(df_comparison['Model'], df_comparison['ROC AUC'], color='teal')
plt.xlabel('ROC AUC Score')
plt.title('ROC AUC Comparison Between Models')
plt.xlim(0.0, 1.05)
plt.gca().invert_yaxis()
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.metrics import confusion_matrix, RocCurveDisplay, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

# Load the data
X_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X_test.csv")
y_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y_test.csv").values.ravel()

# Dictionary of activated models
models = {
    "Logistic Regression": LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1),
    "Naive Bayes": GaussianNB(),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "Decision Tree": DecisionTreeClassifier(random_state=42)
}

# Load training data
X_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X.csv")
y_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y.csv").values.ravel()

# Generate plots
for name, model in models.items():
    print(f"\nüîç Visualizing: {name}")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]

    # 1Ô∏è‚É£ Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap="Blues")
    plt.title(f"Confusion Matrix - {name}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    # 2Ô∏è‚É£ ROC Curve
    RocCurveDisplay.from_estimator(model, X_test, y_test)
    plt.title(f"ROC Curve - {name}")
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.show()

# 3Ô∏è‚É£ Bar chart of ROC AUC scores for all models
print("\nüìä ROC AUC Summary Bar Chart:")
results = []
for name, model in models.items():
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_proba)
    results.append((name, auc))

# Convert to DataFrame
df_auc = pd.DataFrame(results, columns=["Model", "ROC AUC"])
df_auc.sort_values(by="ROC AUC", ascending=False, inplace=True)

# Plot the bar chart
plt.figure(figsize=(10, 6))
plt.barh(df_auc["Model"], df_auc["ROC AUC"], color="darkcyan")
plt.xlabel("ROC AUC Score")
plt.title("ROC AUC Comparison Between Models")
plt.xlim(0.0, 1.05)
plt.gca().invert_yaxis()
plt.grid(axis="x", linestyle="--", alpha=0.5)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
from sklearn.metrics import confusion_matrix, RocCurveDisplay, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

save_path = "Project_Fraud_Detection/Visualizations"
os.makedirs(save_path, exist_ok=True)

X_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X_test.csv")
y_test = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y_test.csv").values.ravel()
X_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/X.csv")
y_train = pd.read_csv("Project_Fraud_Detection/Data/preprocessed_data/Y.csv").values.ravel()

models = {
    "Logistic Regression": LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced', n_jobs=-1),
    "Naive Bayes": GaussianNB(),
    "KNN": KNeighborsClassifier(n_neighbors=5),
    "Decision Tree": DecisionTreeClassifier(random_state=42)
}

roc_results = []

for name, model in models.items():
    print(f"\nüîç Generating visualizations for: {name}")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]

    # 1Ô∏è‚É£ Heatmap (Confusion Matrix)
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap="Purples")
    plt.title(f"Confusion Matrix - {name}")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.tight_layout()
    plt.savefig(f"{save_path}/Confusion_{name.replace(' ', '_')}.png", dpi=300)
    plt.close()

    # 2Ô∏è‚É£ ROC Curve
    disp = RocCurveDisplay.from_estimator(model, X_test, y_test)
    plt.title(f"ROC Curve - {name}")
    plt.grid(True, linestyle='--', alpha=0.6)
    plt.tight_layout()
    plt.savefig(f"{save_path}/ROC_{name.replace(' ', '_')}.png", dpi=300)
    plt.close()

    # 3Ô∏è‚É£ AUC Score
    auc = roc_auc_score(y_test, y_proba)
    roc_results.append((name, auc))

# 3Ô∏è‚É£ Bar Chart for ROC AUC comparison
df_auc = pd.DataFrame(roc_results, columns=["Model", "ROC AUC"])
df_auc.sort_values(by="ROC AUC", ascending=False, inplace=True)

plt.figure(figsize=(10, 6))
plt.barh(df_auc["Model"], df_auc["ROC AUC"], color="darkcyan")
plt.xlabel("ROC AUC Score")
plt.title("ROC AUC Comparison Between Models")
plt.xlim(0.0, 1.05)
plt.gca().invert_yaxis()
plt.grid(axis="x", linestyle="--", alpha=0.5)
plt.tight_layout()
plt.savefig(f"{save_path}/ROC_AUC_Comparison.png", dpi=300)
plt.show()

